<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/dsns.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Harmonizing Structural Cues for Image Compositing using Diffusion Model">
  <meta name="twitter:description" content="Harmonizing Structural Cues for Image Compositing using Diffusion Model">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/dsns.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Harmonizing Structural Cues for Image Compositing using Diffusion Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Harmonizing Structural Cues for Image Compositing using Diffusion Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=m7NIJWMAAAAJ&hl=en" target="_blank"></a><sup></sup></span>
                <a href="https://scholar.google.com.au/citations?user=Qxmqp-0AAAAJ&hl=en" target="_blank"></a><sup></sup></span>
                <a href="https://scholar.google.com/citations?user=rqn08ogAAAAJ&hl=en" target="_blank"></a><sup></sup></span>
                <a href="https://scholar.google.com.au/citations?user=SacY05oAAAAJ&hl=en" target="_blank"></a><sup></sup></span>
                <a href="https://scholar.google.com.au/citations?user=ylX5MEAAAAAJ&hl=en" target="_blank"></a><sup></sup></span>
                <a href="https://scholar.google.co.uk/citations?user=Kj-lB0MAAAAJ&hl=en" target="_blank"></a><sup></sup></span>
                <span class="author-block">
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank"><sup></sup> </a></span>
                <a target="_blank"><sup></sup> </a></span>
                <a target="_blank"><sup></sup> </a></span>
                <br>ICCV 2025
              
              </span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                     

                   

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/objectcomp/Structural-Compositing.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/geod_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Compositing visually coherent foreground and background elements remains a pivotal challenge in generative image editing. Prominent diffusion-based approaches either rely on rigid image-level inputs, limiting scene diversity, or focus on localized mask conditioning (e.g., edges, depth) while struggling to effectively harmonize multiple structural cues. To address these limitations, we propose an advanced diffusion framework that achieves precise foreground-background integration through structured visual cues. Our method introduces a multi-feature encoder and an adaptive feature-gating mechanism to ensure accurate feature extraction and balanced structural fusion. To further enhance contextual consistency, we implement a customized conditional normalization injection block that seamlessly injects these gated features into the diffusion U-Net, enabling context-aware blending and improved foreground-background harmonization. Additionally, we incorporate a training-free pipeline that enables user-defined foreground positioning, scaling, and bounding box generation on-the-fly, providing precise spatial control over object placement. Experimental comparisons demonstrate our framework’s superior compositional fidelity, structural coherence, and style adherence to state-of-the-art. Qualitative evaluations further highlight its effectiveness in handling intricate object structures, context shifts, and diverse scene variations. Our approach offers a scalable and extensible solution for compositional image generation, with broad applications in data augmentation, creative content design, and augmented reality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <!-- Heading for the image -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 12px;">Results: Compositing Multiple Foreground Objects with Structural Guidance</h2>
      </div>

      <div class="item" style="text-align: center;">
        <!-- Increased Image Size -->
        <img src="static/images/two.jpg" alt="Bounding Box Results" 
             style="display: block; margin: auto; width: 95%; max-width: 1400px; height: auto; margin-bottom: 10px;"/>
        
        <!-- Caption below the image -->
        <h2 class="subtitle has-text-justified" style="margin-top: 10px;">
          The above figure presents the results of our method when compositing two foreground objects onto a background image using structural control inputs. Specifically, edge masks and HED masks are utilized to define the structural boundaries of the foreground objects, while a depth mask provides spatial guidance for the background. This multi-condition compositing approach ensures seamless object integration, maintaining structural coherence, depth consistency, and natural blending between the foreground and background elements. The results highlight the effectiveness of our framework in harmonizing multiple structural cues to generate visually coherent composite scenes.          .
        </h2>
      </div>

    </div>
  </div>
</section>



<!-- Custom CSS for Justification and Image Size -->
<style>
  .has-text-justified {
    text-align: justify;
  }
</style>

<style>
  .has-text-justified {
    text-align: justify;
  }
</style>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <!-- Heading for the image -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 12px;">Results: Bounding Box Generation</h2>
      </div>

      <div class="item" style="text-align: center;">
        <!-- Increased Image Size -->
        <img src="static/images/B-box.jpg" alt="Bounding Box Results" 
             style="display: block; margin: auto; width: 95%; max-width: 1400px; height: auto; margin-bottom: 10px;"/>
        
        <!-- Caption below the image -->
        <h2 class="subtitle has-text-justified" style="margin-top: 10px;">
          The above figure illustrates our inference-time bounding box generation results, where structural controls (edge mask, HED mask, and depth masks) and textual prompts guide the synthesis process. Our method ensures precise spatial control, allowing seamless integration of foreground objects into the background. The first row presents results on the <b>IP102 insect dataset</b> composited with leaf images from the <b>PlantVillage dataset</b>, demonstrating realistic object-background harmonization. The second row shows results on the <b>COCO dataset</b>, where objects such as car, bird, and bus are composited within the background scene. Our approach effectively maintains structural integrity, natural blending, and spatial coherence while enabling dynamic object placement and automated bounding box annotation.
        </h2>
      </div>

    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <!-- Heading for the image -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 12px;">Results: Prompt-Guided Image Diversity with Consistent Structural Controls</h2>
      </div>

      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/diversity-.jpg" alt="Bounding Box Results" 
             style="display: block; margin: auto; width: 90%; max-width: 1400px; height: auto; margin-bottom: 10px;"/>
        
        <!-- Caption below the image -->
        <h2 class="subtitle has-text-justified" style="margin-top: 5px;">
          The above figure demonstrates how textual prompts influence image diversity while maintaining the same structural control inputs. By varying the prompt while keeping edge maps, depth maps, or other conditioning inputs unchanged, our method enables flexible scene compositions and style variations. The first row presents results from the <b>COCO dataset</b>, where different prompts guide object placement and scene details, showcasing adaptability in complex settings. The second row features results from the <b>IP102 insect dataset</b> composited with leaf images from the <b>PlantVillage dataset</b>, highlighting our model’s ability to generate realistic insect-leaf interactions with varying textual descriptions. This showcases the model’s capability to integrate semantic guidance while preserving structural integrity, ensuring that different prompts yield diverse yet spatially consistent outputs.
        </h2>
      </div>

    </div>
  </div>
</section>

<style>
  .has-text-justified {
    text-align: justify;
  }
</style>



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <!-- Heading for the image -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 12px;">Approach</h2>
      </div>

      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/method.jpg" alt="Bounding Box Results" 
             style="display: block; margin: auto; width: 90%; max-width: 1400px; height: auto; margin-bottom: 10px;"/>
        
        <!-- Caption below the image -->
        <h2 class="subtitle has-text-justified" style="margin-top: 5px;">
          Architectural overview of the proposed multi-condition diffusion framework for composite image generation. (a)  Pipeline: The model integrates foreground objects (Canny and HED masks) and background (Depth maps) through dedicated encoders and an adaptive gating-based fusion mechanism. Fused features are seamlessly injected into the pre-trained diffusion U-Net via a customized conditional normalization feature injection block. (b) Inference Pipeline: Demonstrates spatial control mechanisms for precise foreground object manipulation, including adjustments in size, scale, and bounding box placement.
        </h2>
      </div>

    </div>
  </div>
</section>

<style>
  .has-text-justified {
    text-align: justify;
  }
</style>





<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/video1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/video2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/video3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>TBA</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
